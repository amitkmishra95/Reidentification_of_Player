# -*- coding: utf-8 -*-
"""reidentification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KFNHdJa1_Z7GG8FFMhl5FIYTt3Ywdz4u
"""

!pip install -q ultralytics opencv-python deep_sort_realtime torch torchvision

import cv2
import numpy as np
from ultralytics import YOLO
from deep_sort_realtime.deepsort_tracker import DeepSort
import os

class YOLODetector:
    def __init__(self, model_path=MODEL_PATH, conf_thresh=0.5):
        self.model = YOLO(model_path)
        self.conf_thresh = conf_thresh

    def detect(self, frame):
        results = self.model(frame, verbose=False)[0]
        detections = []
        for box in results.boxes:
            conf = float(box.conf[0])
            cls = int(box.cls[0])
            if cls != 0 or conf < self.conf_thresh:
                continue
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            detections.append(([x1, y1, x2 - x1, y2 - y1], conf, 'person'))
        print(f"[DETECTION] Filtered {len(detections)} valid people")
        return detections

class Tracker:
    def __init__(self):
        self.tracker = DeepSort(max_age=120,n_init=3 , nn_budget=100,
    max_cosine_distance=0.2 )

    def update(self, detections, frame):
        if not detections:
            return []
        return self.tracker.update_tracks(detections, frame=frame)

def read_video(path):
    return cv2.VideoCapture(path)

def write_video(frame, out_writer):
    out_writer.write(frame)

def init_writer(output_path, frame_shape, fps=30):
    h, w, _ = frame_shape
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    return cv2.VideoWriter(output_path, fourcc, fps, (w, h))

def main():
    VIDEO_PATH =  '/content/sample_data/input/15sec_input_720p.mp4'
    OUTPUT_PATH = '/content/sample_data/output/tracked_outputnew.mp4'
    MODEL_PATH = 'yolov8s.pt'

    cap = cv2.VideoCapture(VIDEO_PATH)
    if not cap.isOpened():
        print("❌ Video not found.")
        return

    detector = YOLODetector()
    tracker = Tracker()

    ret, frame = cap.read()
    if not ret:
        print("❌ Could not read video.")
        return

    writer = init_writer(OUTPUT_PATH, frame.shape)
    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)

    frame_id, total_tracks = 0, 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        detections = detector.detect(frame)
        tracks = tracker.update(detections, frame)

        print(f"[TRACKING] Frame {frame_id} — Tracks: {len(tracks)}")

        for track in tracks:
            if not track.is_confirmed():
                continue
            x1, y1, x2, y2 = map(int, track.to_ltrb())
            track_id = track.track_id
            if (x2 - x1) * (y2 - y1) < 100:  # ignore tiny boxes
                continue

            cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)
            cv2.putText(frame, f'Player {track_id}', (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)
            total_tracks += 1

        writer.write(frame)
        frame_id += 1

    cap.release()
    writer.release()
    print(f"✅ Done. {frame_id} frames processed. Total IDs drawn: {total_tracks}")

main()

